# MemEvolve Environment Configuration
# Copy this file to .env and update the values for your environment

# LLM API Configuration
# Base URL for the LLM API (e.g., Ollama, OpenAI-compatible)
MEMEVOLVE_LLM_BASE_URL=http://192.168.1.61:11434/v1

# API key for the LLM service (leave empty for local services like Ollama)
MEMEVOLVE_LLM_API_KEY=

# Default model to use for LLM operations
MEMEVOLVE_LLM_MODEL=

# Auto-resolve model info for llama.cpp APIs (true/false)
MEMEVOLVE_LLM_AUTO_RESOLVE_MODELS=true

# Embedding API Configuration
# Base URL for the embedding API (e.g., Ollama with embedding models)
MEMEVOLVE_EMBEDDING_BASE_URL=http://192.168.1.61:11435/v1

# API key for the embedding service (leave empty for local services)
MEMEVOLVE_EMBEDDING_API_KEY=

# Default embedding model
MEMEVOLVE_EMBEDDING_MODEL=

# Auto-resolve model info for llama.cpp embedding APIs (true/false)
MEMEVOLVE_EMBEDDING_AUTO_RESOLVE_MODELS=true

# Storage Configuration
# Path for storing memory data
MEMEVOLVE_STORAGE_PATH=./data/memory.json

# Logging Configuration
# Logging level (DEBUG, INFO, WARNING, ERROR)
MEMEVOLVE_LOG_LEVEL=INFO

# Project Configuration
# Project root directory
MEMEVOLVE_PROJECT_ROOT=.

# Data directory
MEMEVOLVE_DATA_DIR=./data

# Cache directory
MEMEVOLVE_CACHE_DIR=./cache