# MemEvolve: Memory-Enhanced LLM API Proxy

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Tests](https://img.shields.io/badge/tests-400+%20passing-brightgreen.svg)](src/tests)

MemEvolve adds persistent memory capabilities to any OpenAI-compatible LLM API. Drop-in memory functionality for existing LLM deployments - no code changes required.

## üî¨ Research Background

This implementation is based on the concepts introduced in the paper:

**MemEvolve: Meta-Evolution of Agent Memory Systems**  
üìÑ [arXiv:2506.10055](https://arxiv.org/abs/2506.10055)  
üë• Authors: Guibin Zhang, Haotian Ren, Chong Zhan, Zhenhong Zhou, Junhao Wang, He Zhu, Wangchunshu Zhou, Shuicheng Yan

If you use MemEvolve in your research, please cite:

```bibtex
@misc{zhang2025memevolvemetaevolutionagentmemory,
      title={MemEvolve: Meta-Evolution of Agent Memory Systems}, 
      author={Guibin Zhang and Haotian Ren and Chong Zhan and Zhenhong Zhou and Junhao Wang and He Zhu and Wangchunshu Zhou and Shuicheng Yan},
      year={2025},
      eprint={2512.18746},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2512.18746}, 
}
```

## üöÄ Features

- **Drop-in Memory**: Add persistent memory to any OpenAI-compatible LLM API without code changes
- **Transparent Proxy**: Your existing applications work unchanged - just change the API URL
- **Smart Context**: Automatically retrieves and injects relevant memories into conversations
- **Learning System**: Captures insights from every interaction to improve future responses
- **Universal Compatibility**: Works with llama.cpp, vLLM, OpenAI API, Anthropic, and any OpenAI-compatible service
- **Production Ready**: Docker deployment, health monitoring, and enterprise-grade reliability
- **Memory Management**: Full API for inspecting, searching, and managing stored memories

## üåü How It Works

1. **Proxy Requests**: MemEvolve sits between your application and your LLM API
2. **Add Context**: Before sending to LLM, retrieves relevant memories from past conversations
3. **Enhanced Responses**: LLM receives conversation history + relevant context
4. **Learn Continuously**: After response, extracts and stores new insights for future use

## üìä Example Enhancement

**Before (Direct LLM):**
```json
{"messages": [{"role": "user", "content": "How do I debug Python memory leaks?"}]}
```

**After (With MemEvolve):**
```json
{
  "messages": [
    {"role": "system", "content": "Relevant past experiences:\n‚Ä¢ Memory profiling with tracemalloc (relevance: 0.89)\n‚Ä¢ GC monitoring techniques (relevance: 0.76)"},
    {"role": "user", "content": "How do I debug Python memory leaks?"}
  ]
}
```

## üöÄ Quick Start (5 minutes)

### 1. Install & Configure

```bash
# Clone and setup
git clone https://github.com/thephimart/memevolve.git
cd memevolve
pip install -r requirements.txt

# Configure your LLM API
cp .env.example .env
# Edit .env - set MEMEVOLVE_UPSTREAM_BASE_URL (embeddings default to same endpoint)
```

### 2. Start MemEvolve Proxy

```bash
# Start the memory-enhanced proxy (auto-reload disabled by default)
python scripts/start_api.py

# For development with auto-reload (shows file change notifications)
python scripts/start_api.py --reload
```

### 3. Point Your Apps to MemEvolve

```python
# Change your existing OpenAI client:
client = OpenAI(
    base_url="http://localhost:11436/v1",  # Was: your-llm-url/v1
    api_key="dummy"  # API key handled by proxy
)
```

**That's it!** MemEvolve automatically adds memory to all your LLM interactions.

### üé® Try the Web Interface

For an immediate demo, use the included Streamlit web interface:

```bash
# In another terminal (MemEvolve server must be running)
cd webui
pip install -r requirements.txt
streamlit run main.py
```

Open `http://localhost:11437` for a chat interface that automatically uses MemEvolve's memory features!

## üì¶ Installation (Detailed)

### Prerequisites
- **Python**: 3.12 or higher
- **LLM API**: Access to any OpenAI-compatible API (vLLM, Ollama, OpenAI, etc.) with embedding support
- **Three API Endpoints** (can be the same service or separate):
  - **Upstream API**: Primary LLM service for chat completions and user interactions
  - **LLM API**: Dedicated LLM service for memory encoding and processing (can reuse upstream)
  - **Embedding API**: Service for creating vector embeddings of memories (can reuse upstream)

### Setup
```bash
git clone https://github.com/thephimart/memevolve.git
cd memevolve
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your API endpoints:
# - MEMEVOLVE_UPSTREAM_BASE_URL (required)
# - MEMEVOLVE_EMBEDDING_BASE_URL (auto-detected for common setups)
```



## üèóÔ∏è How It Works

MemEvolve consists of four core memory components working together:

### Memory Pipeline
```
User Request ‚Üí Memory Retrieval ‚Üí LLM Processing ‚Üí Response + Learning ‚Üí Memory Storage
```

### Components
- **Encode**: Transforms conversations into structured memories (lessons, skills, insights)
- **Store**: Persists memories using vector databases for fast similarity search
- **Retrieve**: Finds relevant memories based on conversation context
- **Manage**: Maintains memory health through pruning and consolidation

### API Requirements
MemEvolve needs AI services for:
- **LLM API**: Chat completions and encoding experiences (e.g., llama.cpp, vLLM, OpenAI)
- **Embedding API**: Vectorizing memories for semantic search (defaults to same as LLM endpoint)

### Smart Integration
- **Context Injection**: Relevant memories added to system prompts
- **Continuous Learning**: Every interaction improves future responses
- **Automatic Management**: Memory stays optimized without manual intervention

## üíæ Component Responsibilities

| Component | Responsibility | Implementation Status |
|-----------|-------------|----------------------|
| **Encode** | Transforms raw experience into structured representations (lessons, skills, tools, abstractions) | ‚úÖ Complete |
| **Store** | Persists encoded information (JSON, vector databases) | ‚úÖ Complete |
| **Retrieve** | Selects task-relevant memory (semantic, hybrid, LLM-guided) | ‚úÖ Complete |
| **Manage** | Maintains memory health (pruning, consolidation, deduplication) | ‚úÖ Complete |



## üß™ Testing

Run the API wrapper test suite:
```bash
pytest src/tests/test_api_server.py -v
```

Run all tests:
```bash
pytest src/tests/ -v
```

Code quality:
```bash
flake8 src/ --max-line-length=100
```

## üìä Current Status

### Implementation Progress
- ‚úÖ **Memory System**: Complete and tested
- ‚úÖ **API Wrapper**: Production-ready proxy server
- ‚úÖ **Memory Integration**: Context injection and learning
- ‚úÖ **Configuration**: Simple .env-based setup
- ‚úÖ **Deployment**: Docker and orchestration support
- ‚úÖ **Documentation**: API wrapper guides and examples
- ‚úÖ **Testing**: 400+ tests covering all functionality

### Test Coverage
- **Total Tests**: 400+
- **API Tests**: 9 comprehensive integration tests
- **Memory Tests**: Full component coverage
- **Performance**: <200ms latency overhead verified

## üìö Documentation

### Getting Started
- **[Getting Started Guide](docs/getting-started.md)**: Complete setup and usage guide
- **[API Reference](docs/api-reference.md)**: All endpoints and configuration options
- **[Deployment Guide](docs/deployment.md)**: Docker and production deployment

### Configuration & Troubleshooting
- **[Configuration Guide](docs/configuration.md)**: Environment setup and options
- **[Troubleshooting Guide](docs/troubleshooting.md)**: Common issues and solutions

### Advanced Topics
- **[Advanced Patterns](docs/tutorials/advanced_patterns.md)**: Complex memory architectures

## üìñ Technical Documentation

- [**PROJECT.md**](PROJECT.md) - Technical architecture and implementation
- [**TODO.md**](TODO.md) - Development roadmap
- [**AGENTS.md**](AGENTS.md) - Development guidelines

## üõ†Ô∏è Development

### Project Structure
```
memevolve/
  ‚îú‚îÄ‚îÄ src/
  ‚îÇ   ‚îú‚îÄ‚îÄ api/             # API wrapper server
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.py    # FastAPI server with proxy endpoints
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes.py    # Memory management endpoints
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware.py # Memory integration middleware
  ‚îÇ   ‚îú‚îÄ‚îÄ components/        # Memory component implementations
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ encode/      # Experience encoding
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ store/       # Storage backends (JSON, vector)
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retrieve/    # Retrieval strategies
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ manage/      # Memory management
  ‚îÇ   ‚îú‚îÄ‚îÄ evolution/        # Meta-evolution framework
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ genotype.py  # Memory architecture representation
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ selection.py # Pareto-based selection
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diagnosis.py # Trajectory analysis
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mutation.py  # Architecture mutation
  ‚îÇ   ‚îú‚îÄ‚îÄ tests/           # Comprehensive test suite
  ‚îÇ   ‚îî‚îÄ‚îÄ utils/           # Configuration, logging, embeddings
  ‚îú‚îÄ‚îÄ scripts/             # Startup and deployment scripts
  ‚îú‚îÄ‚îÄ docs/                # Comprehensive documentation
  ‚îî‚îÄ‚îÄ examples/            # Usage examples
```

### Key Design Principles
- Agent-driven memory decisions
- Hierarchical representations
- Multi-level abstraction
- Stage-aware retrieval
- Selective forgetting

## ü§ù Contributing

This is a private repository. For development:
1. Create a feature branch: `git checkout -b feature/your-feature`
2. Make your changes
3. Run tests: `pytest src/tests/ --timeout=600 -v`
4. Commit with descriptive messages
5. Push to branch: `git push origin feature/your-feature`

## üìù License

MIT License - See LICENSE file for details

## üìß Contact

- **Repository**: https://github.com/thephimart/memevolve
- **Issues**: https://github.com/thephimart/memevolve/issues

## üîó Related Resources

- [PROJECT.md](PROJECT.md) - Detailed architecture and implementation status
- [TODO.md](TODO.md) - Development roadmap
- [AGENTS.md](AGENTS.md) - Development guidelines